Contained here is the arduino code that runs a digital synthesizer in which the notes played are selected by the position of the player's eyes, or eye. The body emits various biological signals. The eyes have polarity, which means that we can monitor the position of the eye based on a change in voltage. This means, that we can use our eyes to control things, including audio. To do this, one electrode must be connected to the outer canthus of an eye, the other can be connected to either the outer canthus of the other eye, or to the mastoid bone on the opposite side, and finally, a ground lead should be attached to the forehead. The method of capturing this singal is known as EOG or Electrooculography. Note that different reference placements will result in different outcomes. The signals here are small, in the microvolts range, which is far too small for the arduino to be able to make use of. To solve this problem, I built a signal amplifier using an INA111 instrumentation amp and and a LM324 op amp along with an offset ground(arduino can't read negative voltages, so this is necessary). In this version, changing the position of the eyes changes the selection of notes along the pentatonic scale. The scale can be configured in many different ways, the ratios can be changed, and you could add MIDI output. You can also make note selection conditional using if statements, (ie if left, play A, if more left play B, if up play C, if right play scale, etc.), which would require calibration by reading the serial monitor values at different eye positions (everyone is going to be different, and electrode placement absolutely matters here).

Now beyond synthesizers, this tech has utility in a number of applications, especially if the electrodes could be made to be part of a pair of glasses, including gesture controlled robotics, advanced prosthetics, smart home control, vehicle safety (detect drowsiness, sleeping, etc.), and of course, music and entertainment. 

This is a work in progress. Future improvements will include using both eyes as input, which will require the construction of an additional amplifier circuit and figuring out how to share the offset ground with both amps (I am not an electrical engineer but I will figure this out), ability to switch scales and play modes using buttons, switches, and/or pots. Some of the cool parts about having both eyes as input(and referencing to opposite side mastoid) is that there will be increased accuracy when implementing conditional note selection choices and we could also add multiple outputs. Possibilities are endless. We can also add multiple outputs for one input, meaning that if we wanted to, we could use the eyes to output chords, or each eye could independently control selection from the pentatonic map if we are running this without conditional statements.

This could be altered to also have EMG(electromyography) as input. This signal is usually a bit stronger and more consistent. EMG electrodes could be placed on a forearm, and EOG electrodes on the eyes, and with EOG controlling notes and EMG controlling things like phase, effects, drum sounds, octave, or even a second pitch, a player could make some really cool sounds by just looking around and squeezing their fist. 

I may create another version of this that runs off of the Teensy 4.0. Long term plan could be to feed signals into an arduino or teensy for the ADC and then into RasberriPi, where machine learning could pick up on patterns and make the note selection very consistent and accurate. 

Authored and built by Jacob Sutherland.
I must give credit where credit is due. The design for the signal amplifier largely comes from watchmeflyy's "Controlling Lights With Your Eyes" project at https://www.instructables.com/Controlling-Lights-With-Your-Eyes/ (Many thanks for making this available!!!), and the audio design and code structure makes use of some of Peter Knight's Audiono http://code.google.com/p/tinkerit/wiki/Auduino (Again, many many thanks!). 
